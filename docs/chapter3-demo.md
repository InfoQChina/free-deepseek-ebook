# DeepSeek 颠覆了什么？学习不靠“人盯”，AI 自己“卷”自己

### DeepSeek 的最大功绩在于将这一切透明化

##### InfoQ：“DeepSeek 坚持纯强化学习路线，但业界常说 RL（强化学习）是‘炼丹’”——他们如何让这个过程可控和“平民化”？有什么"推理范式的创新"？



**李维博士：** 实际上，推理模型的强化学习一直是业界的难题。大约半年前，IIya 等人宣称预训练时代已经结束，这意味着单纯依靠预训练模型的规模扩展来提高性能已经难以为继。GPT-5 迟迟不能上线也是预训练式微的一个迹象。因此，业界开始寻找新的增长道路，推理大模型在头部团队开始暗流涌动，直到 Open AI 发布全球第一个推理大模型 O1. 紧接着就是 DeepSeek 的 R1 出圈，这就是 Deepseek 爆火的背景。



从神神秘秘、据传引发了 OpenAI 宫斗的 Q-Star 项目开始到 o1 大模型的推出，推理大模型被 AI 主流广泛公认为新的范式。这种范式的核心是开启模型的“慢思考”能力，即所谓 System 2，利用强化学习提升模型在复杂任务中的智能程度。然而，这一切都是闭源的，OpenAI 甚至故意制造了一些神秘感，遮掩其思维链的内容。除了少数头部玩家如 Google 和 Anthropic 在背后悄悄探索追踪外，其他团队对这一领域知之甚少。



[DeepSeek ](https://qcon.infoq.cn/2025/beijing/presentation/6331?utm_source=infoqweb&utm_medium=homegeobanner)的 **最大功绩在于将这一切透明化。它的模型和详尽的技术论文全部开源，甚至也不怕露怯，在系统里公开了思维链的所有内容**。它通过纯粹强化学习，证明了即使没有过程控制数据，仅通过结果控制也能达到头部推理大模型的水平。这就好像是捅破了一层窗户纸，让业界看到了强化学习平民化的道路。

##### InfoQ：推理范式的创新听起来很抽象，能否举个例子？



**李维博士：**DeepSeek 的 R1 论文非常出色，堪称大模型领域中的一篇佳作。论文分为两部分：**一部分是关于 Zero 的研究**，这是纯粹的强化学习推理方向的成果，非常精彩；**另一部分则是基于 Zero 研究成果的实用系统 R1**，这是一个真正上线的头部推理大模型。在开发 R1 时，需要考虑实用性，包括综合性能、安全性以及各种实用考量等，因此论文中详细介绍了四阶段训练的最佳实践（best practice），帮助其他团队理解和复制这一成果。



论文最精彩的部分还是 Zero 的研究。Zero 的研究证明了一个颠覆性的观点：**与传统认知（或 OpenAI 所暗示的需要在每一步监督推理强化学习的观点）不同，实际上并不需要过程监督**。仅通过最终结果（即“黄金标准”）作为监督信号，就能训练出推理大模型所需的“慢思考”过程。



这是 Zero 的最大亮点，也是其名称的由来——它借鉴了 AlphaZero 的精神。AlphaZero 在人工智能历史上开创性地完全不依赖人类棋谱或经验学习，而是通过自我对弈的再生的过程数据（即：棋局状态 + 落子 + 评分的三元组步骤数据），实现了零人类监督的强化学习，并最终完全碾压了人类顶尖棋手。DeepSeek 的 Zero 研究也是如此，它表明在推理任务中，模型可以自主生成内部的过程数据，即思维链（CoT，Chain of Thought）序列，而无需人类标注。



具体来说，推理模型最初以数学和代码为对象，因为这些领域本身就存在标准答案。从宏观上看，这其实是一种典型的端到端监督学习，因为输入端（数学题或代码题）和输出端（答案或代码运行结果）都是固定的、已知的。然而，从输入到输出的过程非常复杂，信息差很大，这就需要一个“思维链”作为桥梁。就像人类遇到难题时需要分解问题、逐步思考一样，模型也需要这样的过程。DeepSeek 的研究发现，模型本身具有自主学习这种深度思考过程的能力，只要给予足够的时间和空间。如果没有这个空间，模型就只能直接从问题跳到答案，信息鸿沟大，随机性就强，成绩好不了。



**DeepSeek 的解决方案是通过设计一个简单模板引导模型进行思考**。具体说，就是在传统的监督数据 question+answer 里面人为增加了一个标签 [think]: question+[think]+answer, 通过强化学习的方式，模型会自主填空，再生过程数据 question+CoT+answer，以此迭代学习，CoT 中就自动出现了反思、自我校正等过程。这表明，只要给予模型思考的空间，它就能自主生成思维链。非常奇妙！

### 给模型留够充分的自主学习空间

##### InfoQ：动态推理路径听起来像 AI 自己“画思维导图”——但如何避免它中途跑偏？比如写代码时突然开始写诗？



**李维博士：** 从目前的情况来看，这种可能性几乎不存在，或者概率极低，可以忽略不计。在 Deepseek 公布他们的结果和研究细节之前，大家确实对这一点感到困惑：只靠结果监督，没有过程监督，深度思维不会乱套吗。在没有真正进行大规模强化学习实验之前，这确实是一个很大的疑问。就好比放风筝，你只牵着一根线，让风筝在天上自由飞翔，你会担心它会不会一头栽到地上。



现在看来是过虑了。它不会走偏的原因在于，所有这些推理的强化学习，包括自主生成的推理思维链的数据，实际上都是建立在原有的头部大模型（如 V3）的基础上的。这些大模型在海量数据的学习过程中，已经很好地掌握了如何把话说得顺溜。这种“顺溜”的背后是条理性。虽然不能说它完全等同于逻辑性，但至少不会偏离到完全不合理的情况。就像一个人说话很顺畅，背后的思想相对来说也是有条理的。



所以，**模型在原有大模型的基础上生成数据，经过筛选和强化学习迭代，会越来越条理化**。这种思考方式本身是由大模型自然生成的，再加上有选择机制在不断强化过程中让它越来越符合条理地导向正确答案。

话说回来，在研究人员真正做出成果之前，大家心里还是充满了怀疑和疑问，不知道让机器模拟学习人类的高阶智能这条路是否真的能走通。如果是一个能力弱的小模型，这条路是否能走通就很难说了。但 V3 本身是一个很强大的基座模型，在此基础上让模型自己生成思维链，虽然这些思维链并不总是很有条理，但并不影响最终结果。因为这是一个以结果为导向的强化学习过程，只要坚持用正确和错误的结果来控制强化学习过程，即使思维链中有时会出现一些偏差，但总体目标是一致的，最终还是能学到推理高难度题目的能力。



再从更大的角度来看，我们发现当大模型发展到一定程度时，日常人类的数据已经基本用尽，高品质的数据也所剩无几。要进一步提升能力，就必须依靠模型自己生成数据。说到底，**AI 发展到现在，需要 AI 自己反哺自己才能进一步提升**。



在过去很长一段时间里，很多人对这一点存在疑问，担心模型自己教自己会导致退化，或者即使是一个好的模型教一个差的模型，也会有天花板。但现在回过头来看，**再生数据的重要性越来越大**。不仅是推理模型，就连多模态大模型也是如此。以 Sora 为例，我们知道视频和语言之间的自然对齐数据非常少，很难找到大量对视频情节进行详细讲解的数据。为了实现视频和语言的对齐，Sora 选择了再生数据的道路，用自己的模型对整个的视频训练数据集进行了非常详细的标注。再生数据助力，Sora 成为了第一个爆款的视频大模型。如今，国内的视频大模型也已经迎头赶上，如快手的可灵和字节的即梦，甚至比 Sora 还要更强一些，这背后也离不开再生数据的作用。

##### InfoQ：另一方面，与 OpenAI 的 o1 相比，DeepSeek R1 还有一个显著亮点是将推理思维链应用到了语言文字的创作和风格模仿能力上，这一点可以详细介绍一下吗？



**李维博士：**[o1 ](https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651219062&idx=2&sn=093259f994cef33992f9e3aeaf1b7fc2&scene=21#wechat_redirect)出来时，大家都知道它在数学和代码能力上有了显著提升，因为标准测试显示它达到了一个更高的水平。但大家没有意识到的是，这种推理能力，或者说“慢思维”能力，不仅仅在需要严格逻辑推理的领域表现出色，它在传统的语言文字创作方面同样可以大放异彩。



传统上，语言文字能力一直是大模型的强项，大家都知道大模型生成的语言非常流畅。到了像 4o 或 V3，它们写文章已经很顺了，似乎提升空间不大。然而，当要求模型写一篇古典诗歌，或者模仿鲁迅的文风时，之前的模型还做不到。直到 R1 推出，这些问题都得到了解决。从社会效应来看，这其实是非常厉害的。



老实说，真正关心数学或代码的人并不多，虽然我们知道代码是今后几年的一个大方向，自动编程能改变世界。所有 IT 方面的东西归根结底都是软件，数字世界是由软件构成的。如果软件能力可以从手工编写变成模型辅助，甚至模型自主编写，这将极大地提高我们的生产力。这是大家都能看到的，但对普通老百姓来说却没有那么直观，因为他们面对的更多是写文章如何出彩这类任务。



**当 R1 的文科能力被大家发现后，不仅仅是极客或者做软件应用的人看到了推理模型的好处，普通人也为之奔走相告**。一旦上手，任何人都可以成为诗人、文学家、哲学家，这种震撼是非常大的。在 o1 出来时，大家没有这种感觉，可能是因为 OpenAI 没有意识到，或者至少没有聚焦这一点。但 DeepSeek 在做代码和数学推理时，内部肯定已经意识到，这种“慢思维”在文字能力方面也可以提升一大步，尤其是在中文领域。



大家都知道，中文的数据相对没有英文那么丰富，所以之前大模型写英文诗可以写得很漂亮，但写唐诗就不够好。这可能是因为中文数据要么量不够，要么品质不够，导致模型学习得不够到位。我们一直觉得这是一个遗憾，模型写诗有时押韵，有时不押韵，有时多一个字，少一个字，更不用说平仄，总是有问题。DeepSeek 在这方面肯定下了功夫，其数据品质一定比行业标准更高、更好。



但大模型光有数据还不够，**另一条腿是推理时间的计算量**。在用户实际使用时，增加计算量和思考时间，我们发现模型的文字能力显著提升了层次，这给大家的震撼非常大。思维链是模型“慢思考”的一个特征。一开始，我们可能想当然地认为，逻辑思维是它的核心，思维链就是要非常严谨地符合逻辑的每个步骤，以确保在数理化和代码中表现出色。



但我们根本没想到，在文学创作这种领域，并不需要严谨的逻辑思维，它更多的是要有想象力，需要反复斟酌和修改。比如你要写一篇非常漂亮的文章，或者模仿一种风格，你需要考虑的方面很多，写古风诗词要考虑押韵、平仄、用词，考虑如何用古典文字表达现代概念等。为了写出一篇好文章，你需要周密地计划，这本质上是一种“planning”，而不仅仅是狭义的“reasoning”。**可见，慢思维背后的真正价值在于为最终结果做铺垫，制定计划和反复修正。** 无论任务是文科还是理科，只要是高难度的任务，都需要这种“planning”的时间，就像我们打草稿、反复校改一样，这些都是思维链的用武之地。

##### InfoQ：思维链机制具体是如何产生的？



**李维博士：**DeepSeek 之所以能够产生复杂的思维链，背后是因为它是基于头部大模型 V3 训练的，而 V3 所涵盖的知识比我们任何个体所了解的都要广博得多得多。在这基础上，关键点是要给模型留下空间，让它有自主学习的机会。作为设计者或开发者，需要设计出这样的空间，让模型自己去填补、去学习。DeepSeek 就是这样实现的。它设计了一种格式，在输入问题 question 和输出答案 answer 之间，它留下了一个“思考”的空间，用标签 [think] 来标记: question+[think]+answer。这个 think 标签就是准备要学   思维链（CoT） 的, 虽然开始为空，Zero 的 research 表明：只要留下 think 的标签，就给 LLM 自主填补 CoT 留下了空间。此后他们“啊哈”地惊喜发现，越来越条理化的 CoT 居然在 GRPO 组内选优的强化学习迭代算法的指引下，就自主学出来了。啥也不用做，模型就是自己要思考，而且能思考。LLM really wants/tends to think and think deep if given a chance.  比如，它可能会在推理过程中发现自己前面的某个结论与已知事实不符，于是就会自我纠正，说：“不对，这里可能有偏差。”这种反思和自我纠正的能力，是模型在学习过程中自然形成的。可以想像研究者当时的兴奋之情, 简直就是上帝给他们面授了天机。不但他们“啊哈”, 我们读论文追踪他们的人也感觉开了天目，不可思议，但 it just works。Zero research 的美丽就是没有人工的过程数据的任何干预，完完全全的纯强化出来的奇迹。



从信息论的角度来说，[思维链](https://www.infoq.cn/article/DbTFg0GOHUwbVIOjebLo?utm_campaign=geek_search&utm_content=geek_search&utm_medium=geek_search&utm_source=geek_search&utm_term=geek_search)降低了困惑度（perplexity），搭建了从难题到答案之间的桥梁，使得得出正确结论的可能性增大，从而提高了模型的智能。

### 推理模型已经进入“平民化”时代

##### InfoQ：如果让您给程序员推荐一个最值得复现的 DeepSeek 模块，会是哪个？比如各种声称几十美元复制 R1 的 Aha moment？



**李维博士：** 如果让我推荐程序员群体最值得复现的 DeepSeek 模块，大概会是与 **Zero 研究相关的部分**。这种复现并不是从全面能力上，而是证实了 Zero 研究中揭示的关键点——机器确实能够自主学到反思能力或慢思维推理。这是 OpenAI 一直遮掩不让人知道的，也许他们早就悟出来了，但就是不公开。



现在，我们看到至少有五六组不同的团队，用很少的资源就复现出了 R1 的这种反思能力。这不仅是一个有趣的实验，更关键的是，它标志着 **推理模型已经进入“平民化”时代**。以前，大家不知道推理模型是如何工作的，只知道需要大量的过程数据，模型才能学会慢思维。这被认为是一个难以跨越的门槛，因为过程数据很难获取，而且强化学习的不稳定性高、对数据要求也高，所以很多程序员觉得这条路很难走。



但现在，我们知道可以绕过这个最繁难的过程数据，通过有限的资源复现这种“Aha moment”，证明慢思维能力是可以让模型自主学出来的。基于这个前提，如果你是一个行业专家（domain expert），在自己的项目或应用领域中，你会想：是否可以用这些技术在你的领域实现大幅提升？这是完全可能的。因为即使是最强大的大模型（如 V3 或 4o），在具体场景中如果不经过优化，也只能达到 60%~70% 的正确率，而在 real life 应用场景中，经验告诉我们没有 80% 或 85% 以上的正确率，根本无法上线一个真正有价值的系统。



从大模型的“开箱即用”（out-of-box）结果到真正能投入应用并产生价值，中间存在一个差距。以前，我们想到的唯一方法是收集领域数据进行微调。但现在，我们多了一条路：**顺着推理模型的思路，让系统充分发挥推理阶段的慢思维能力，从而提升数据质量到可接受甚至出彩的程度**。这条路似乎已经打通了。

不过，我的码农朋友告诉我，他做了一个微调（SFT）与 Deepseek 式强化学习（RL）的对比实验，发现 RL 的确强过 SFT，但 RL 训练目前的计算代价还是远远大于 SFT。效果好于 SFT 可以理解，因为 SFT 的数据总是非常有限的，而 RL 自主再生的数据成功强化的话，会远远大于 SFT 数据。



仔细看 R1 的设计，它是一个实用系统，不像 Zero 那么纯粹。Zero 是一个研究项目，旨在证明可以排除人类干预来构建推理模型。但 R1 是为了实际应用，所以它结合了微调和强化学习：遵循他们自己创新的 SFT+RL+SFT+RL 的四阶段训练的 pipeline。它在第一阶段是微调，使用了 2,000 条左右的人类过程数据来提高效率，他们称为“冷启动”。强化学习之后，又加入了微调和最后的偏好强化学习，以确保合适的数据配比和能力平衡，以及与人类偏好的对齐。这种设计是经过深思熟虑，可能经过了很多尝试和调整，最终呈现出的一个最佳实践。



虽不好说 R1 的这种设计一定就是绝对的最佳方案，但它确实提供了一个很好的思路：**现在我们有两个工具——SFT 和 RL**。如果能够将这两个工具很好地结合起来，互相补充，那么在实际应用场景中，我们就能构建出更好的系统。



从更广泛的意义上说，DeepSeek 的出现不仅是因为各种原因而短暂火爆，**它更重要的作用是极大地加速了大模型向应用领域发展的速度**。这对整个行业来说是一个巨大的利好刺激。

##### InfoQ：有人说大模型是“暴力美学”，但 OpenAI 的前首席科学家、联合创始人 IIya 说预训练到头了，怎么讲？推理模型出现的背景就是增加了又一个暴力美学的 scaling law 吗？



**李维博士：** 这更像是技术聚焦点的转移和技术创新的范式转变。大模型涉及三大块：**首先是预训练**，这是大模型的基础能力，从海量数据中学习基本规律；**其次是后训练**，最初主要是微调，OpenAI 早期也用了一些强化学习（如 RLHF）来对齐人类偏好，但到了 Meta 时，他们甚至放弃了典型的 RLHF，代之以更简单的 DPO，因为与很多人一样，他们玩不转。**最后是推理阶段的工作**，即模型上线后与用户交互的阶段。



这三个阶段理论上都可能找到资源投入与性能提升之间的正相关 S 曲线，即 scaling laws 的某种表现函数。在过去，预训练是最受重视的部分，大家认为只要数据量不断加大、模型规模足够大，能力就一定持续提升。



[LLM Scaling](https://www.infoq.cn/article/gruzolGMLzijxWy63XDs?utm_campaign=geek_search&utm_content=geek_search&utm_medium=geek_search&utm_source=geek_search&utm_term=geek_search) 的底层逻辑是什么？为什么到了千亿 tokens 这种以前难以想象的数据规模，大模型依然显得"吃不饱"？为什么从千亿扩展到万亿 tokens，scaling law 依然有效？



这个现象的关键在于 LLM 是序列学习（编码）和序列推理（解码）的系统。序列本身是一维的，但序列中蕴含的 patterns 和规律性却是高维的。举个例子：即使是简单的"猫追老鼠"这样的序列，背后可能涉及物种关系、捕食行为、空间运动等多个维度的知识。这种多维知识表现在序列层面，就会发生天然的组合爆炸。对大数据的"大胃口"正是应对这种组合爆炸的有效策略。



然而，人类自然产生的高质量数据是有限的。预训练已经几乎吃尽了现有的高质量自然数据。业界开始意识到数据增长的困扰，性能提升也变得困难。GPT-5 难产，据传投入大量算力却收效有限，这表明 **预训练可能遭遇了瓶颈**。



于是，业界开始探索另外的 AI 智能增长曲线。强化学习的推理模型就是在这种背景下走到主流舞台的中心：应该 **在后训练中加入纯粹的强化学习**。以前的强化学习依赖人类偏好，但这次是让模型在得出答案之前有更多思考时间，学习背后的规律。V3 已经做得很好，但当时除了业界并没有在社会上引起太大轰动。直到 R1 出现，Deepseek 才真出圈了，成了春节后最受关注的大众话题，在海外也引发了热议和震惊。R1 代表了一种新的范式。在 R1 之前，只有 OpenAI 出了 o1 这种推理模型，给人一种高不可攀的感觉，大家不知道如何跟进。然而，R1 不仅复现了 o1 的能力，还更加透明、清晰。这种反差进一步凸显了 R1 作为开源大模型引领者的重要性。

### 未来脑洞

##### InfoQ：DeepSeek 乍看就是工程上的极致化，为什么会引起全世界的轰动？它的获客速度（一周上亿）超过了 ChatGPT 核爆的时候？它的历史地位到底如何？



**李维博士：** 从我个人的体会和感受来说，大模型的发展历程中，ChatGPT 的爆火是一个标志性事件。其实我们业内人在 ChatGPT 出现之前就开始关注大模型了，至少从 GPT-3  开始吧。当时 GPT-3 的 Playground 出现，我们乐在其中，就已经感觉到一场风暴要来了。但从整个社会的感知来看，真正引发全社会震动的还是 ChatGPT 的出现，它像核爆一样震撼了我们，超出了所有人的预期。ChatGPT 出来，我们就陷入了一种痴迷的状态。



R1 的 出现，我认为是继 ChatGPT 之后的第二个重大震撼。当然，在 ChatGPT 和 R1 之间也出现了一些有影响力的大模型，比如 4o，它也是一个了不起的里程碑。我们当时觉得 ChatGPT 已经很好了，3.5 版本已经很出色了，但 4o 的出现证明了它还可以更好。我们一直在案头使用它。再后来出现了 Sora，这种视频大模型也给人带来了震撼。我个人还特别喜欢一个叫 Suno 的音乐模型，它在音乐创作方面表现出色，让我觉得自己仿佛一夜之间就能成为音乐家，想写什么歌就写什么歌，还能配上自己的视频。这些模型都给人带来了不同阶段的震撼，但都没有 R1 这么强烈。



如果让我排序的话，我认为 R1 的震撼力仅次于 ChatGPT，甚至超过了 4o 和 Sora 所创造的轰动效应。R1 的震撼感有点类似于当年 ChatGPT 刚出现时的感觉，让人痴迷。ChatGPT 是开天辟地的大模型，R1 总体上是一个追随者，尽管它有很多创新亮点，有些方面甚至超越了之前的模型，比如在古典诗词创作和文风模仿方面。作为追随者，能在太平洋两岸乃至全球引起如此大轰动，是奇迹般的成就。



从实际效果来看，R1 的产品化非常成功。它在一周内就获得了上亿客户，远远打破了 ChatGPT 所创造的记录，提升了整个社会对 AI 的感知度。此外，从地缘政治对技术应用的影响来看，国内很多用户一直渴望使用全世界最先进的大模型，比如 GPT 系列、Claude 或 Gemini，但常常够不着。而 R1 的出现，让人们不用担心国内外的限制。这些也都是促成 R1 快速普及的因素。

##### InfoQ：您理想中 AI 编程的终极形态是什么？是程序员对着 AI 说“给我做个抖音”，它就直接输出可部署的代码 + 运维方案吗



**李维博士：总是有两类人**：**怀疑派和乐观派**。像 Ilya 这样的人，认为通用人工智能（AGI）已经迫在眉睫，超级智能（ASI）也在不远的未来，**所以现在最大的问题是确保超级智能的安全性**。



Anthropic 的 CEO 预计，在未来 3 到 5 年内，大模型将实现真正的突破，不仅仅是目前让我们震撼的表现和 demos，而是真正能在生产力上对整个社会带来革命性的改变。他们所说的，归根结底就是 **AI 能规模化平替人类的体力劳动和脑力劳动**。目前大模型虽然很热闹，但在社会生活中的实际应用还远未达到上一代移动互联网平台的水平。上一代的 super apps，比如美团、滴滴、小红书、抖音等，它们改变了我们日常生后的主要方面，无论吃穿住行还是通信和娱乐，它们最大程度缩短了供应商和客户之间的距离，这些价值我们每天都能感受到。而玩大模型虽然有趣，但在生活层面的实际价值还不明显，应用层面还处于爆发的前夕。



值得指出的是，DeepSeek 的出现降低了大模型应用门槛，为应用铺平了道路，虽然目前我们还没有进入应用真正爆发的时代。未来，当 AI 应用真正爆发时，会是什么时候、什么样子呢？我认为，最终目标是 AI 在脑力劳动和体力劳动中全面代替人类。大模型对白领阶层的冲击，迹象已经很明显，甚至连程序员群体都难幸免。体力劳动方面，具身智能发展也很快，无论是人形机器人还是机械手，都在逐步代替人类的体力劳动。



当然，这也会带来副作用，比如大量工作岗位消失，社会如何适应这种生产力大发展但缺乏工作岗位的状态，是另一个层面的讨论。但从 AI 本性和最终目标来看，**AI 的发展可以有两个里程碑：一是何时能替代人类 50% 的工作**，让社会只需要一半人工作，剩下的人通过基本收入保障（UBI）等方式维持一个体面的自由生活，在我看来这就是 AGI 到老的标志；**二是何时能替代 90% 的人类工作**，这可能算是所谓的超级智能（ASI）出现的时候，某种意义上的技术共产主义。

[![官网下载](https://img.shields.io/badge/📖-立即下载完整电子书-FF6F61?style=for-the-badge)](https://www.infoq.cn/minibook/4cryFPs6rVbU02ALhpt8)
